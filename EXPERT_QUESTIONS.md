# Questionamentos e Pesquisas para Especialistas em IA

## Perguntas Abertas para Melhorar o Artigo de Pneumonia com Transfer Learning e Ensemble

---

## üìã Contexto do Projeto

**T√≠tulo**: Transfer Learning e Ensemble Learning para Classifica√ß√£o de Pneumonia em Raios-X Tor√°cicos

**Resultados Atuais**:

- EfficientNet-B0: 80.29% acc, 99.74% sens, 47.86% spec
- Ensemble (Simple/Weighted Voting): 71.47% acc - N√ÉO superou individual
- Dataset: 5,863 imagens (desbalanceamento 1:2.89)
- Problema cr√≠tico: 52% de falsos positivos

**Objetivo deste documento**: Coletar insights de especialistas em IA/ML para tornar o artigo mais completo, rigoroso e interessante para publica√ß√£o em venues de alto impacto (MICCAI, TMI, MedIA).

---

## üî¨ SE√á√ÉO 1: METODOLOGIA E DESIGN EXPERIMENTAL

### 1.1 Arquitetura e Transfer Learning

**Q1.1**: Por que EfficientNet-B0 superou ResNet-50 e DenseNet-121 em 13-14%?

- Nossa hip√≥tese: Compound scaling √© mais eficiente que residual/dense connections
- **Pergunta para especialistas**:
  - H√° evid√™ncias na literatura de que compound scaling √© superior para imagens m√©dicas?
  - Seria mais apropriado comparar com modelos de capacidade similar (params)?
  - ResNet-50 (25.6M) vs EfficientNet-B0 (5.3M) - compara√ß√£o justa?

**Q1.2**: Progressive Unfreezing - estrat√©gia ideal?

- Usamos: 5 √©pocas classifier-only + 20 √©pocas full fine-tuning
- **Pergunta para especialistas**:
  - Essa propor√ß√£o (1:4) √© padr√£o? Literatura sugere outras?
  - Dever√≠amos usar gradual unfreezing (camada por camada) em vez de two-stage?
  - Layer-wise Learning Rate Decay (LLRD) seria melhor? Ex: LR √ó 0.95^(max_layer - current_layer)

**Q1.3**: ImageNet Pre-training vs Medical Pre-training

- Todos os modelos foram pr√©-treinados em ImageNet (fotos naturais)
- **Pergunta para especialistas**:
  - Vale a pena pr√©-treinar em ChestX-ray14 ou MIMIC-CXR antes de fine-tunar?
  - Self-supervised (SimCLR, MoCo, MAE) em raios-X seria mais eficaz?
  - H√° estudos comparando ImageNet vs Medical pre-training para pneumonia?

**Q1.4**: Arquiteturas modernas n√£o testadas

- N√£o testamos: Vision Transformers (ViT, Swin, BEiT), ConvNeXt, MaxViT
- **Pergunta para especialistas**:
  - ViT requer datasets maiores (literatura sugere >10K). Com 5.2K samples, overfittaria?
  - Swin Transformer seria mais apropriado (hierarchical, menor receptive field inicial)?
  - Hybrid CNN-Transformer (ConvNeXt, CoAtNet) seria o melhor dos dois mundos?

---

### 1.2 Ensemble Learning

**Q2.1**: Por que Simple Voting = Weighted Voting (resultados id√™nticos)?

- Pesos calculados por AUC: 34.26%, 33.36%, 32.38% (diferen√ßa de apenas 2%)
- **Pergunta para especialistas**:
  - Isso indica que os modelos t√™m performance similar demais para weighted voting funcionar?
  - Dever√≠amos usar m√©tricas mais discriminativas para pesos? (Ex: Specificity, Balanced Acc)
  - Pesos adaptativos por imagem (confidence-based) seriam melhores?

**Q2.2**: Por que ensemble n√£o superou o melhor individual (-8.82%)?

- Nossa an√°lise: domin√¢ncia de modelos fracos + correla√ß√£o de erros + pesos ineficazes
- **Pergunta para especialistas**:
  - Essa falha de ensemble √© comum em imagens m√©dicas? H√° casos similares na literatura?
  - Diversity metrics (Q-statistic, correlation coefficient, disagreement) deveriam ser calculados antes?
  - Negative Correlation Learning (NCL) durante treinamento for√ßaria diversidade?

**Q2.3**: Stacking vs Voting - qual implementar?

- Planejamos implementar stacking com meta-learner (Logistic Regression, XGBoost, LightGBM)
- **Pergunta para especialistas**:
  - Qual meta-learner √© mais robusto para datasets m√©dicos pequenos?
  - Devemos treinar meta-learner em validation set ou usar cross-validation nested?
  - Feature engineering no meta-learner (ex: concatenar [predictions, confidence, disagreement]) ajuda?

**Q2.4**: Ensemble de qu√™? CNN + Transformer?

- Atualmente: 3 CNNs (EfficientNet, ResNet, DenseNet) - arquiteturas similares
- **Pergunta para especialistas**:
  - Ensemble de CNN + Vision Transformer teria mais diversidade?
  - Ensemble de modelos com diferentes resolu√ß√µes (224√ó224, 384√ó384, 512√ó512)?
  - Ensemble de modelos treinados com diferentes augmentations?

---

### 1.3 Loss Functions e Balanceamento

**Q3.1**: Focal Loss - hiperpar√¢metros ideais

- Planejamos: Œ±=[1.945, 0.673], Œ≥=2.0
- **Pergunta para especialistas**:
  - Œ≥=2.0 √© o padr√£o de Lin et al. (2017), mas h√° estudos tuning Œ≥ para imagens m√©dicas?
  - Œ± deveria ser igual aos class weights ou ajustado separadamente?
  - Focal Loss funciona melhor com qual optimizer? (Adam vs AdamW vs SGD)

**Q3.2**: Class-Balanced Loss vs Focal Loss

- Temos duas op√ß√µes para lidar com desbalanceamento
- **Pergunta para especialistas**:
  - Class-Balanced Loss (CB Loss) √© superior a Focal Loss em datasets m√©dicos?
  - Combinar ambos (CB-Focal Loss) como em Cui et al. (2019) faz sentido?
  - LDAM Loss (Label-Distribution-Aware Margin) seria melhor?

**Q3.3**: Outros approaches para desbalanceamento

- N√£o testamos: SMOTE, Undersampling, Cost-sensitive learning expl√≠cito
- **Pergunta para especialistas**:
  - SMOTE (oversampling sint√©tico) funciona bem em imagens m√©dicas?
  - Undersampling da classe majorit√°ria (jogar fora dados) √© recomendado?
  - Two-stage training (balance primeiro, imbalance depois) ajudaria?

---

### 1.4 Data Augmentation

**Q4.1**: Augmentation espec√≠fico para raio-X

- Planejamos: CLAHE, Elastic Deformation, Grid Distortion, etc.
- **Pergunta para especialistas**:
  - H√° augmentations espec√≠ficos para raio-X que n√£o estamos considerando?
  - RandAugment ou AutoAugment (busca autom√°tica) funcionam em imagens m√©dicas?
  - Augmentation baseado em f√≠sica (simula√ß√£o de diferentes energias de raio-X)?

**Q4.2**: Mixup/CutMix em imagens m√©dicas

- Mixup interpola imagens; CutMix cola regi√µes
- **Pergunta para especialistas**:
  - Mixup √© v√°lido clinicamente? (misturar pneumonia + normal ‚Üí diagn√≥stico amb√≠guo)
  - CutMix preserva melhor anatomia que Mixup?
  - Mosaic augmentation (grid de 4 imagens) usado em YOLO seria √∫til?

**Q4.3**: Test-Time Augmentation - quantas augmentations?

- Planejamos: 5 augmentations (horizontal flip, rotate ¬±5¬∞, shift, brightness)
- **Pergunta para especialistas**:
  - 5 √© suficiente ou 10-20 seria melhor? (trade-off tempo vs performance)
  - Quais augmentations s√£o mais eficazes em TTA para raio-X?
  - TTA com voting (modal class) vs averaging (probabilidades) - qual melhor?

---

## üéØ SE√á√ÉO 2: AVALIA√á√ÉO E M√âTRICAS

### 2.1 M√©tricas de Performance

**Q5.1**: Balanced Accuracy vs F1-Score - qual priorizar?

- Balanced Acc = (Sens + Spec) / 2
- F1 = 2 √ó (Prec √ó Rec) / (Prec + Rec)
- **Pergunta para especialistas**:
  - Para aplica√ß√£o cl√≠nica de screening, qual m√©trica √© mais informativa?
  - H√° m√©tricas espec√≠ficas para imbalanced medical imaging (ex: G-mean)?
  - Matthews Correlation Coefficient (MCC) seria melhor que F1 para imbalance?

**Q5.2**: Threshold Optimization - qual m√©todo?

- Planejamos: Youden's J, F1-max, Target-Specificity, Cost-Sensitive
- **Pergunta para especialistas**:
  - Para pneumonia, qual m√©todo √© clinicamente mais justific√°vel?
  - Como definir custos em cost-sensitive? Custo(FN) = 10 √ó Custo(FP) √© razo√°vel?
  - Threshold deve ser otimizado por fold (cross-validation) ou global?

**Q5.3**: Calibra√ß√£o de probabilidades

- N√£o analisamos calibra√ß√£o (reliability diagrams, ECE, Brier score)
- **Pergunta para especialistas**:
  - Calibra√ß√£o √© cr√≠tica para aplica√ß√£o cl√≠nica? (m√©dicos confiam em "90% de certeza"?)
  - Temperature Scaling, Platt Scaling ou Isotonic Regression - qual melhor para calibrar?
  - Focal Loss naturalmente descalibra modelos? (foco em hard examples)

---

### 2.2 Valida√ß√£o Estat√≠stica

**Q6.1**: Testes estat√≠sticos - estamos usando os corretos?

- Usamos: McNemar's test (accuracy), Bootstrap CI (intervalos de confian√ßa)
- **Pergunta para especialistas**:
  - DeLong's test para comparar AUCs seria mais apropriado?
  - Paired t-test para m√∫ltiplas m√©tricas (F1, Balanced Acc)?
  - Bonferroni correction para m√∫ltiplas compara√ß√µes (5 modelos)?

**Q6.2**: Cross-validation - nested ou n√£o?

- Planejamos: K=5 stratified CV (outer loop)
- **Pergunta para especialistas**:
  - Nested CV (inner loop para tuning de hiperpar√¢metros) √© necess√°rio?
  - GroupKFold seria melhor? (agrupar imagens do mesmo paciente)
  - Repeated CV (5√ó2 ou 10√ó10) aumenta confiabilidade? Trade-off com custo computacional?

**Q6.3**: Signific√¢ncia cl√≠nica vs estat√≠stica

- Diferen√ßa estatisticamente significativa (p<0.001) nem sempre √© clinicamente relevante
- **Pergunta para especialistas**:
  - Qual diferen√ßa m√≠nima de Specificity √© clinicamente importante? (5%, 10%, 15%?)
  - Non-inferiority test seria apropriado? (ensemble n√£o √© inferior a individual)
  - Como reportar efeito pr√°tico al√©m de p-value? (Cohen's d, odds ratio)

---

### 2.3 Interpretabilidade e Explicabilidade

**Q7.1**: Grad-CAM - suficiente para valida√ß√£o cl√≠nica?

- Planejamos: Grad-CAM++ para visualizar regi√µes importantes
- **Pergunta para especialistas**:
  - Grad-CAM √© o estado da arte ou h√° m√©todos melhores? (LayerCAM, ScoreCAM, XGrad-CAM)
  - Attention maps de Transformers s√£o mais interpret√°veis que Grad-CAM de CNNs?
  - Como validar quantitativamente? (Intersection over Union com m√°scaras de radiologistas?)

**Q7.2**: Attention mechanisms - devem ser adicionados?

- N√£o usamos: Self-Attention, Channel Attention (SE, CBAM), Spatial Attention
- **Pergunta para especialistas**:
  - Attention modules melhoram interpretabilidade e performance simultaneamente?
  - Squeeze-and-Excitation (SE) j√° est√° no EfficientNet - suficiente?
  - Attention Branch Network (ABN) que for√ßa aten√ß√£o durante treino seria √∫til?

**Q7.3**: Saliency maps e perturbation-based methods

- N√£o testamos: RISE, LIME, SHAP, Integrated Gradients
- **Pergunta para especialistas**:
  - Esses m√©todos s√£o complementares a Grad-CAM ou redundantes?
  - SHAP (SHapley Additive exPlanations) √© mais rigoroso matematicamente?
  - Perturbation tests (ocluir regi√µes, medir drop de performance) s√£o necess√°rios?

---

## üè• SE√á√ÉO 3: APLICA√á√ÉO CL√çNICA E VALIDA√á√ÉO

### 3.1 Trade-off Sensitivity-Specificity

**Q8.1**: Qual threshold para aplica√ß√£o real?

- Atualmente: Sens=99.74%, Spec=47.86%
- Meta: Sens‚â•95%, Spec‚â•65%
- **Pergunta para especialistas**:
  - Em screening de pneumonia pedi√°trica, qual trade-off √© aceit√°vel clinicamente?
  - H√° guidelines de sociedades m√©dicas (ATS, ERS, RSNA) sobre thresholds m√≠nimos?
  - Different thresholds para diferentes contextos? (emergency vs routine screening)

**Q8.2**: Impacto de falsos positivos vs falsos negativos

- FP: Paciente normal recebe tratamento desnecess√°rio (antibi√≥ticos, exames adicionais)
- FN: Paciente com pneumonia n√£o √© tratado (risco de morte)
- **Pergunta para especialistas**:
  - Como quantificar custos de FP vs FN? (financeiro, tempo, qualidade de vida)
  - An√°lise de decis√£o (decision curve analysis) seria apropriada?
  - Net Benefit metric para avaliar utilidade cl√≠nica?

**Q8.3**: Compara√ß√£o com performance humana

- N√£o comparamos com radiologistas (inter-rater agreement)
- **Pergunta para especialistas**:
  - Como obter baseline de radiologistas? (anotar subset do test set?)
  - Comparar com radiologista individual ou consenso de 3+ especialistas?
  - AI deve superar ou ser "n√£o-inferior" a humanos? (regulatory perspective)

---

### 3.2 Generaliza√ß√£o e Robustez

**Q9.1**: External validation - quais datasets?

- Nosso dataset: Guangzhou (China), pedi√°trico (1-5 anos)
- **Pergunta para especialistas**:
  - Quais datasets p√∫blicos s√£o apropriados para valida√ß√£o externa?
    - ChestX-ray14 (adultos, multi-patologia)
    - MIMIC-CXR (adultos, hospital √∫nico)
    - PadChest (adultos, Espanha)
    - VinBigData (adultos, Vietn√£)
  - Zero-shot generalization (testar sem re-treinar) ou fine-tune?
  - Qu√£o diferente pode ser a popula√ß√£o e ainda ser considerado "external validation"?

**Q9.2**: Robustness testing - quais perturba√ß√µes?

- Planejamos: Gaussian noise, contrast reduction, rotation
- **Pergunta para especialistas**:
  - H√° perturba√ß√µes espec√≠ficas de raio-X? (scatter, beam hardening, grid artifacts)
  - Adversarial attacks (FGSM, PGD) s√£o relevantes para imagens m√©dicas?
  - Robustness benchmarks estabelecidos? (ImageNet-C, ImageNet-A equivalentes para medical)

**Q9.3**: Domain shift e dataset bias

- Nosso dataset pode ter biases (equipamento, protocolo, popula√ß√£o)
- **Pergunta para especialistas**:
  - Como detectar dataset bias quantitativamente?
  - Domain adaptation techniques (DANN, CORAL) deveriam ser usados?
  - Multi-source domain generalization (treinar em m√∫ltiplos hospitais) √© vi√°vel?

---

### 3.3 Aspectos Regulat√≥rios e √âticos

**Q10.1**: Regulatory approval - FDA, CE Mark, ANVISA

- Sistema seria classificado como Class II ou III medical device?
- **Pergunta para especialistas**:
  - Quais evid√™ncias s√£o necess√°rias para approval? (prospective study, RCT?)
  - 510(k) pathway (equival√™ncia a dispositivo existente) seria aplic√°vel?
  - Software as Medical Device (SaMD) guidelines espec√≠ficos?

**Q10.2**: Fairness e vi√©s demogr√°fico

- N√£o analisamos performance por: sexo, idade, etnia, severidade
- **Pergunta para especialistas**:
  - An√°lise de subgrupos √© obrigat√≥ria? (disparate impact)
  - Como garantir fairness quando dados demogr√°ficos n√£o est√£o dispon√≠veis?
  - Fairness metrics (demographic parity, equalized odds) aplicam-se a medical AI?

**Q10.3**: Explicabilidade para stakeholders

- Diferentes stakeholders: radiologistas, m√©dicos generalistas, pacientes, reguladores
- **Pergunta para especialistas**:
  - N√≠vel de explica√ß√£o varia por stakeholder? (heatmap para m√©dico, "90% certeza" para paciente)
  - Counterfactual explanations ("se essa opacidade fosse menor, seria normal") s√£o √∫teis?
  - Right to explanation (GDPR) aplica-se a AI m√©dica?

---

## üí° SE√á√ÉO 4: INOVA√á√ÉO E CONTRIBUI√á√ïES

### 4.1 Novelty e Originalidade

**Q11.1**: O que torna nosso trabalho novel?

- Nossa an√°lise: EfficientNet superior, ensemble underperformance, trade-off analysis
- **Pergunta para especialistas**:
  - Compara√ß√£o de 3 arquiteturas em setup controlado √© contribui√ß√£o suficiente?
  - An√°lise de _por que_ ensemble falha √© mais interessante que simplesmente reportar?
  - H√° gap na literatura que estamos preenchendo especificamente?

**Q11.2**: Como enquadrar ensemble underperformance?

- Pode ser visto como: resultado negativo, li√ß√£o aprendida, ou insight importante
- **Pergunta para especialistas**:
  - Journals/conferences aceitam bem "negative results"? (ex: ensemble n√£o funcionou)
  - Devemos enquadrar como "quando NOT usar ensemble" (prescriptive guidance)?
  - Meta-an√°lise de ensemble failures em medical imaging seria paper separado?

**Q11.3**: Contribui√ß√µes metodol√≥gicas vs aplicadas

- Metodol√≥gica: Compara√ß√£o de t√©cnicas, an√°lise de falhas
- Aplicada: Sistema funcional para pneumonia
- **Pergunta para especialistas**:
  - Venues de alto impacto (MICCAI, TMI) preferem metodologia ou aplica√ß√£o?
  - "Better mousetrap" (85% vs 80% acc) √© suficiente ou precisa de inova√ß√£o t√©cnica?
  - Framework generaliz√°vel (aplic√°vel a outras doen√ßas) aumenta impacto?

---

### 4.2 Compara√ß√£o com Estado da Arte

**Q12.1**: Como comparar com Kermany et al. (2018) que atingiu 92.8%?

- Nossa accuracy: 80.29% (EfficientNet) vs 92.8% (Kermany Inception-v3)
- Nossa sensitivity: 99.74% vs 93.2% (superamos!)
- Nossa specificity: 47.86% vs 90.1% (muito inferior)
- **Pergunta para especialistas**:
  - Diferen√ßas de split de dados invalidam compara√ß√£o direta?
  - Dever√≠amos re-implementar m√©todo de Kermany com nosso split?
  - Como reportar compara√ß√£o quando n√£o h√° c√≥digo/splits oficiais?

**Q12.2**: Benchmarks e leaderboards

- N√£o h√° leaderboard oficial para Chest X-Ray Pneumonia dataset
- **Pergunta para especialistas**:
  - Vale a pena criar leaderboard (ex: via Papers With Code)?
  - Standardized splits e evaluation protocol deveriam ser propostos?
  - Como garantir reprodutibilidade? (seeds, hardware, vers√µes de bibliotecas)

**Q12.3**: Compara√ß√£o multi-dataset

- Nosso modelo s√≥ foi testado em um dataset
- **Pergunta para especialistas**:
  - √â v√°lido comparar modelos treinados/testados em datasets diferentes?
  - Meta-analysis agregando resultados de m√∫ltiplos papers seria √∫til?
  - Transfer learning cross-dataset (treinar em A, testar em B) √© avalia√ß√£o melhor?

---

### 4.3 Impacto e Relev√¢ncia

**Q13.1**: Quem se beneficia deste trabalho?

- Pesquisadores, cl√≠nicos, desenvolvedores de sistemas, reguladores, pacientes
- **Pergunta para especialistas**:
  - Para maximizar impacto, qual audi√™ncia priorizar no paper?
  - Resultados negativos (ensemble failure) interessam a pesquisadores mas n√£o cl√≠nicos?
  - Framework open-source + modelos pr√©-treinados aumentam cita√ß√µes?

**Q13.2**: Deployment viability

- Nossa solu√ß√£o √© vi√°vel para deployment real?
- **Pergunta para especialistas**:
  - EfficientNet-B0 (5.3M params) √© leve o suficiente para edge devices? (tablets, mobile)
  - Quantiza√ß√£o (INT8) mant√©m performance? (TensorRT, ONNX Runtime)
  - Cloud vs edge deployment - qual mais apropriado para pneumonia screening?

**Q13.3**: Socioeconomic impact

- Pneumonia √© prevalente em pa√≠ses de baixa renda (sub-Saharan Africa, Southeast Asia)
- **Pergunta para especialistas**:
  - Como adaptar modelo para settings com recursos limitados?
  - Offline models (sem internet) s√£o cr√≠ticos para regi√µes remotas?
  - Cost-effectiveness analysis deveria fazer parte do paper?

---

## üìä SE√á√ÉO 5: APRESENTA√á√ÉO E COMUNICA√á√ÉO

### 5.1 Estrutura do Paper

**Q14.1**: Ordem e √™nfase das se√ß√µes

- Atualmente: Abstract, Intro, Methods, Results, Discussion, Conclusion
- **Pergunta para especialistas**:
  - Ensemble failure deve estar em "Results" ou "Discussion"?
  - Trade-off analysis merece se√ß√£o separada ou integrar em "Results"?
  - Limitations devem ser se√ß√£o separada ou final de "Discussion"?

**Q14.2**: Visualiza√ß√µes e figuras

- Planejamos: ROC curves, bar charts, confusion matrix, Grad-CAM
- **Pergunta para especialistas**:
  - Quais figuras s√£o essenciais vs nice-to-have?
  - Saliency maps de casos corretos vs incorretos (error analysis) adicionam valor?
  - Diagrams de arquitetura (flowcharts, network diagrams) s√£o esperados?

**Q14.3**: Suplementary material

- O que colocar no paper vs supplement?
- **Pergunta para especialistas**:
  - Detalhes de hyperparameter tuning v√£o para supplement?
  - Ablation studies (remover componentes, medir impacto) s√£o suplementares?
  - Additional experiments (ViT, Mixup) que n√£o cabem no main paper?

---

### 5.2 Reprodutibilidade

**Q15.1**: Checklist de reprodutibilidade

- NeurIPS, ICLR, ICML t√™m checklists obrigat√≥rios
- **Pergunta para especialistas**:
  - MICCAI, TMI, MedIA t√™m checklists similares?
  - Quais informa√ß√µes s√£o _essenciais_ para reprodu√ß√£o? (seeds, hardware, library versions)
  - Docker container + scripts √© suficiente ou c√≥digo linha-por-linha?

**Q15.2**: C√≥digo e modelos - onde hospedar?

- Op√ß√µes: GitHub, Papers With Code, Hugging Face, Zenodo
- **Pergunta para especialistas**:
  - GitHub √© suficiente ou deve ter DOI (Zenodo)?
  - Pre-trained weights devem ser disponibilizados? (copyright, size)
  - Interactive demo (Gradio, Streamlit) aumenta impacto?

**Q15.3**: Dados - quest√µes de privacidade

- Dataset √© p√∫blico (Kaggle) mas tem restri√ß√µes?
- **Pergunta para especialistas**:
  - Re-distribuir data splits (train/val/test indices) viola ToS do Kaggle?
  - Synthetic data generation (GANs) para augmentar dataset √© eticamente aceit√°vel?
  - Federated learning (treinar sem centralizar dados) seria alternativa?

---

### 5.3 Escrita e Linguagem

**Q16.1**: Tom e estilo

- Paper deve ser: t√©cnico, acess√≠vel, ou h√≠brido?
- **Pergunta para especialistas**:
  - Medical imaging venues preferem jarg√£o m√©dico ou ML-friendly language?
  - Abstract deve focar em m√©trica (80% acc) ou impacto ("reduz sobrecarga de radiologistas")?
  - Primeira pessoa ("we propose") ou passivo ("it is proposed")?

**Q16.2**: Claims e assertiveness

- Qu√£o forte podem ser as afirma√ß√µes?
- **Pergunta para especialistas**:
  - "EfficientNet is superior" vs "EfficientNet shows promising results" - qual aceito?
  - Claims sobre aplica√ß√£o cl√≠nica sem valida√ß√£o prospectiva - permitido?
  - Diferen√ßa entre "demonstrates", "suggests", "indicates" - importa?

**Q16.3**: Limita√ß√µes - qu√£o honestos ser?

- Temos muitas limita√ß√µes (dataset pequeno, valida√ß√£o externa, etc.)
- **Pergunta para especialistas**:
  - Listar todas as limita√ß√µes enfraquece o paper ou demonstra rigor?
  - Como balancear honestidade vs "selling" o trabalho?
  - Reviewers penalizam se n√£o mencionarmos limita√ß√µes √≥bvias?

---

## üéØ SE√á√ÉO 6: VENUE E PUBLICA√á√ÉO

### 6.1 Target Venue

**Q17.1**: Qual venue √© mais apropriado?

- Op√ß√µes:
  - **Conferences**: MICCAI, IPMI, CVPR Medical Workshop, NeurIPS Medical
  - **Journals**: IEEE TMI, Medical Image Analysis, Computer Methods in Biomedicine, JMIR
- **Pergunta para especialistas**:
  - Para primeiro paper, conference ou journal √© melhor? (timeline, prest√≠gio)
  - MICCAI: mais competitivo mas mais visibilidade?
  - Journal: mais espa√ßo para detalhes mas menos networking?

**Q17.2**: Timeliness e deadline

- MICCAI 2026: deadline ~mar√ßo 2026
- TMI: rolling submission
- **Pergunta para especialistas**:
  - 4 meses (nov‚Üímar) √© suficiente para melhorias + escrita + revis√µes?
  - Submit early (draft inicial) ou wait (ap√≥s todas as melhorias)?
  - Preprint (arXiv) antes ou depois de submission?

**Q17.3**: Resubmission strategy

- Se rejeitado, como adaptar?
- **Pergunta para especialistas**:
  - Feedback de reviewers deve guiar pr√≥ximas experi√™ncias?
  - Resubmit para venue similar ou pivot para aplica√ß√£o diferente?
  - Quanto tempo esperar entre submissions? (ethical implications)

---

### 6.2 Review Process

**Q18.1**: Common reviewer concerns

- Com base em experi√™ncia, quais obje√ß√µes esperar?
- **Pergunta para especialistas**:
  - "Dataset is small" - como rebater? (cross-validation, data augmentation)
  - "No external validation" - √© blocker ou limitation aceit√°vel?
  - "Ensemble failure" - ser√° visto como fraqueza ou insight?

**Q18.2**: Rebuttal strategies

- Como responder a cr√≠ticas durante rebuttal?
- **Pergunta para especialistas**:
  - Adicionar experi√™ncias durante rebuttal period (1-2 semanas) √© vi√°vel?
  - Tone: defensivo vs colaborativo vs agradecido?
  - Quais cr√≠ticas s√£o "deal-breakers" vs negoci√°veis?

**Q18.3**: Revision scope

- Se aceito com major revisions, quanto trabalho adicionar?
- **Pergunta para especialistas**:
  - External validation pode ser adicionada em revision? (2-3 meses)
  - Treinar novos modelos (ViT) conta como "within scope" ou "new paper"?
  - Como negociar com area chair se reviewer pede muito?

---

## üß™ SE√á√ÉO 7: EXPERIMENTOS ADICIONAIS

### 7.1 Ablation Studies

**Q19.1**: Quais componentes ablacionar?

- Progressive unfreezing, class weights, data augmentation, early stopping
- **Pergunta para especialistas**:
  - Ablation de todos os componentes √© necess√°rio ou subset representativo?
  - Como reportar? (tabela com checkmarks) ou (gr√°ficos de impacto)
  - Ablation deve usar best model (EfficientNet) ou all models?

**Q19.2**: Hyperparameter sensitivity

- LR, batch size, epochs, weight decay, dropout
- **Pergunta para especialistas**:
  - Grid search vs random search vs Bayesian optimization - qual reportar?
  - Sensitivity plots (performance vs hyperparameter) s√£o esperados?
  - Optimal hyperparameters s√£o dataset-specific ou generaliz√°veis?

---

### 7.2 An√°lises Adicionais

**Q20.1**: Error analysis - o que analisar?

- Casos dif√≠ceis, failure modes, confusion entre bacterial vs viral
- **Pergunta para especialistas**:
  - An√°lise qualitativa (mostrar imagens) ou quantitativa (caracter√≠sticas de erros)?
  - Clustering de erros (UMAP, t-SNE) revela padr√µes?
  - Radiologist annotation de casos incorretos (por que modelo errou)?

**Q20.2**: Feature analysis

- Visualizar embeddings, ativa√ß√µes, feature importance
- **Pergunta para especialistas**:
  - Feature space analysis (PCA, t-SNE) adiciona insights?
  - CKA (Centered Kernel Alignment) para comparar representa√ß√µes entre modelos?
  - Probing classifiers para entender o que cada camada aprende?

**Q20.3**: Confidence calibration

- Reliability diagrams, Expected Calibration Error (ECE)
- **Pergunta para especialistas**:
  - Calibra√ß√£o √© subestimada em medical imaging papers?
  - Overconfidence vs underconfidence - qual √© pior clinicamente?
  - Selective prediction (abstain em casos incertos) deveria ser implementado?

---

## üìö SE√á√ÉO 8: LITERATURA E CONTEXTO

### 8.1 Related Work

**Q21.1**: Quais papers s√£o must-cite?

- EfficientNet (Tan & Le), ResNet (He et al.), DenseNet (Huang et al.)
- Kermany et al. (mesmo dataset), Rajpurkar CheXNet, Wang ChestX-ray14
- **Pergunta para especialistas**:
  - H√° survey papers de medical image analysis que devemos citar?
  - Seminal works de ensemble learning (Dietterich, Breiman)?
  - Recent works (2023-2025) em pneumonia detection?

**Q21.2**: Posicionamento vs literatura

- Como posicionar nosso trabalho?
- **Pergunta para especialistas**:
  - "First to compare EfficientNet vs ResNet vs DenseNet for pneumonia" - defens√°vel?
  - "First to analyze why ensemble fails" - h√° precedentes?
  - Citation searching: backward (references) vs forward (cited by) - qual priorizar?

---

### 8.2 Future Work

**Q22.1**: Quais dire√ß√µes propor?

- Multi-task (pneumonia + severity + etiology), multi-modal (CXR + CT + clinical data)
- **Pergunta para especialistas**:
  - Future work deve ser realistic (podemos fazer) ou aspirational (algu√©m deveria fazer)?
  - Specific (treinar ViT) vs vague (explore deep learning)?
  - Quantas dire√ß√µes propor? (3? 5? 10?)

**Q22.2**: Longitudinal e temporal analysis

- Tracking pneumonia progression over time (m√∫ltiplas imagens do mesmo paciente)
- **Pergunta para especialistas**:
  - Temporal models (RNNs, Transformers) para sequ√™ncias de raios-X?
  - Predicting treatment response vs apenas diagn√≥stico?
  - Clinical decision support system (CDSS) - pr√≥ximo passo l√≥gico?

---

## üí¨ SE√á√ÉO 9: PERGUNTAS META

### 9.1 Sobre este Documento

**Q23.1**: Relev√¢ncia das perguntas

- Algumas perguntas podem ser irrelevantes ou muito espec√≠ficas
- **Pergunta para especialistas**:
  - Quais destas perguntas s√£o CR√çTICAS para responder antes de submeter?
  - Quais s√£o interesting-to-know mas n√£o blockers?
  - H√° perguntas importantes que n√£o inclu√≠mos?

**Q23.2**: Prioriza√ß√£o

- N√£o temos tempo para responder todas as 70+ perguntas
- **Pergunta para especialistas**:
  - Top 10 perguntas que mais impactam qualidade do paper?
  - Quais podem ser respondidas com literatura vs experimentos?
  - Quais podem ficar como "future work" sem enfraquecer o paper?

---

### 9.2 Colabora√ß√£o e Feedback

**Q24.1**: Coautoria

- Este trabalho pode se beneficiar de coautores especialistas?
- **Pergunta para especialistas**:
  - Radiologista como coautor (valida√ß√£o cl√≠nica) √© necess√°rio?
  - Estat√≠stico (an√°lise rigorosa) agregaria valor?
  - Como abordar potenciais colaboradores? (via email, conference, Twitter/X)

**Q24.2**: Peer feedback informal

- Antes de submiss√£o formal, buscar feedback
- **Pergunta para especialistas**:
  - Lab reading groups s√£o √∫teis? (apresentar draft, receber cr√≠ticas)
  - Postar em Twitter/X, LinkedIn, Reddit (r/MachineLearning) para feedback?
  - Preprint em arXiv - vantagens (feedback early) vs desvantagens (scooping)?

---

## üéì SE√á√ÉO 10: RECURSOS E REFER√äNCIAS

### Para Especialistas que Responderem

**Agradecemos imensamente seu tempo e expertise!**

Suas respostas ajudar√£o a:

- ‚úÖ Tornar o paper mais rigoroso metodologicamente
- ‚úÖ Posicionar contribui√ß√µes de forma mais impactante
- ‚úÖ Evitar erros comuns e obje√ß√µes de reviewers
- ‚úÖ Direcionar experimentos futuros de forma mais eficiente

**Como responder**:

1. Escolha perguntas de seu dom√≠nio de expertise
2. N√£o precisa responder todas - qualquer insight √© valioso!
3. Referencie papers, datasets, c√≥digo se poss√≠vel
4. Indique n√≠vel de confian√ßa (opini√£o vs consenso da √°rea)

**Formato sugerido**:

```
Q[n√∫mero]: [Pergunta]
R: [Sua resposta]
Confian√ßa: [Alta/M√©dia/Baixa]
Refer√™ncias: [Paper/Link se aplic√°vel]
```

---

## üì¨ Contato e Contribui√ß√µes

**Documento vivo**: Este documento ser√° atualizado conforme recebermos respostas e novos insights.

**Para contribuir**:

- GitHub Issue: [link do reposit√≥rio]
- Email: [seu email]
- Twitter/X: [seu handle]

**Pr√≥ximos passos ap√≥s feedback**:

1. Compilar respostas em FAQ
2. Priorizar experimentos baseados em consenso
3. Atualizar paper draft
4. Iterar com especialistas antes de submission

---

**Vers√£o**: 1.0  
**Data**: 14 de Novembro de 2025  
**Autores**: Matheus Borges (+ colaboradores)  
**Status**: Aberto para feedback

**Keywords**: Transfer Learning, Ensemble Learning, Medical Image Analysis, Pneumonia Detection, Deep Learning, EfficientNet, Expert Consultation

---

## üìù Ap√™ndice: Resumo das √Åreas de Questionamento

1. **Metodologia** (Q1-Q4): Arquitetura, ensemble, loss functions, augmentation
2. **Avalia√ß√£o** (Q5-Q7): M√©tricas, valida√ß√£o estat√≠stica, interpretabilidade
3. **Aplica√ß√£o** (Q8-Q10): Trade-offs cl√≠nicos, generaliza√ß√£o, aspectos regulat√≥rios
4. **Inova√ß√£o** (Q11-Q13): Novelty, compara√ß√£o com SOTA, impacto
5. **Comunica√ß√£o** (Q14-Q16): Estrutura, reprodutibilidade, escrita
6. **Publica√ß√£o** (Q17-Q18): Venue selection, review process
7. **Experimentos** (Q19-Q20): Ablations, an√°lises adicionais
8. **Literatura** (Q21-Q22): Related work, future directions
9. **Meta** (Q23-Q24): Prioriza√ß√£o, colabora√ß√£o

**Total**: 24 √°reas principais, 70+ perguntas espec√≠ficas

---

## üôè Agradecimentos Antecipados

Agradecemos antecipadamente a:

- Pesquisadores em Medical Image Analysis
- Especialistas em Deep Learning e Computer Vision
- Radiologistas e profissionais de sa√∫de
- Revisores e membros de program committees
- Comunidade open-source (PyTorch, timm, Albumentations)

**Seu conhecimento √© essencial para elevar a qualidade e impacto deste trabalho!**
