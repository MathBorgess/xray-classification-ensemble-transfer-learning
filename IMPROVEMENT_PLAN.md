# Plano de Melhorias - Redu√ß√£o de Falsos Positivos e Aumento de Acur√°cia

## üìä An√°lise do Problema Atual

### Situa√ß√£o Atual (EfficientNet-B0 - Melhor Modelo)

- ‚úÖ **Accuracy**: 80.29%
- ‚úÖ **AUC**: 0.9761 (excelente)
- ‚úÖ **Sensitivity**: 99.74% (quase perfeito)
- ‚ùå **Specificity**: 47.86% (CR√çTICO - 52% de falsos positivos)
- ‚ùå **122 falsos positivos** de 234 casos normais

### Diagn√≥stico do Problema

**Por que temos tantos falsos positivos?**

1. **Desbalanceamento severo** (1:2.89 ratio Normal:Pneumonia)

   - Modelo aprende vi√©s para classe majorit√°ria (Pneumonia)
   - Class weights (1.945/0.673) n√£o foram suficientes

2. **Loss function inadequada** (CrossEntropyLoss standard)

   - N√£o foca em exemplos dif√≠ceis
   - Trata todos os erros igualmente

3. **Threshold fixo** (0.5)

   - N√£o otimizado para balancear Sensitivity/Specificity
   - Favorece classe com maior probabilidade m√©dia

4. **Augmentation limitado**

   - Apenas horizontal flip
   - N√£o simula varia√ß√µes reais de imagem m√©dica

5. **Valida√ß√£o pequena** (16 samples)

   - Early stopping inst√°vel
   - Pode ter parado longe do √≥timo

6. **Ensemble fraco**
   - Modelos correlacionados (todos CNNs em ImageNet)
   - Weighted voting ineficaz (pesos quase iguais)

---

## üéØ Objetivos de Melhoria

### Metas Quantitativas

- **Specificity**: 47.86% ‚Üí **‚â• 65%** (reduzir FP de 122 para ~82)
- **Accuracy**: 80.29% ‚Üí **‚â• 85%**
- **Sensitivity**: Manter **‚â• 95%** (m√°ximo 19 FN)
- **Balanced Accuracy**: 73.80% ‚Üí **‚â• 80%**

### Estrat√©gia

1. **Reduzir falsos positivos** √© prioridade #1
2. **Manter sensibilidade alta** (custo de FN √© cr√≠tico)
3. **Melhorar generaliza√ß√£o** (cross-validation)

---

## üìã TASKS - Implementa√ß√£o Recomendada

### üî¥ PRIORIDADE ALTA (Impacto Imediato - 1-2 semanas)

#### TASK 1: Threshold Optimization ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Encontrar threshold √≥timo para Specificity ‚â• 65% mantendo Sensitivity ‚â• 95%

**M√©todos a implementar**:

1. **Youden's Index**: $J = \text{Sensitivity} + \text{Specificity} - 1$
2. **F1-Score Maximization**: Otimizar balanceamento Precision-Recall
3. **Target Specificity**: Fixar Spec=65%, encontrar threshold
4. **Cost-Sensitive**: Custo(FN)=10, Custo(FP)=1 (ajustar por contexto cl√≠nico)

**Implementa√ß√£o**:

```python
# threshold_optimization.py
def optimize_threshold(y_true, y_probs, method='youden'):
    thresholds = np.linspace(0, 1, 1000)
    best_threshold = 0.5
    best_score = 0

    for threshold in thresholds:
        y_pred = (y_probs >= threshold).astype(int)

        if method == 'youden':
            score = sensitivity + specificity - 1
        elif method == 'target_spec':
            if specificity >= 0.65:
                score = sensitivity
        # ...

    return best_threshold
```

**Tempo estimado**: 2-3 dias
**Impacto esperado**: Specificity +15-20%, Sensitivity -2-4%
**Risco**: Baixo (n√£o requer re-treinamento)

---

#### TASK 2: Focal Loss Implementation ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Focar aprendizado em exemplos dif√≠ceis (hard negatives)

**Teoria**:
$$FL(p_t) = -\alpha_t(1-p_t)^\gamma \log(p_t)$$

onde:

- $p_t = p$ se $y=1$, sen√£o $1-p$
- $\gamma = 2.0$ (focusing parameter) - reduz peso de exemplos f√°ceis
- $\alpha_t$ = class weight (1.945 para Normal, 0.673 para Pneumonia)

**Por que funciona?**

- Exemplos f√°ceis (bem classificados): $(1-p_t)^\gamma \approx 0$ ‚Üí peso baixo
- Exemplos dif√≠ceis (mal classificados): $(1-p_t)^\gamma \approx 1$ ‚Üí peso alto
- For√ßa modelo a aprender casos lim√≠trofes (onde ocorrem FP/FN)

**Implementa√ß√£o**:

```python
# src/losses.py
class FocalLoss(nn.Module):
    def __init__(self, alpha=None, gamma=2.0):
        super().__init__()
        self.alpha = alpha  # [1.945, 0.673] para nosso dataset
        self.gamma = gamma

    def forward(self, inputs, targets):
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = (1 - pt) ** self.gamma * ce_loss

        if self.alpha is not None:
            alpha_t = self.alpha[targets]
            focal_loss = alpha_t * focal_loss

        return focal_loss.mean()
```

**Modifica√ß√£o no treinamento**:

```python
# train.py
alpha = torch.tensor([1.945, 0.673]).to(device)
criterion = FocalLoss(alpha=alpha, gamma=2.0)
```

**Tempo estimado**: 1-2 dias implementa√ß√£o + 8-10 horas re-treinamento
**Impacto esperado**: Specificity +8-12%, Balanced Acc +5-7%
**Risco**: M√©dio (requer re-treinamento completo)

---

#### TASK 3: Cross-Validation (K=5 Stratified) ‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Valida√ß√£o robusta + usar 100% dos dados de treino

**Problemas atuais**:

- Valida√ß√£o = 16 samples (0.3% do dataset!)
- Early stopping baseado em conjunto min√∫sculo
- Desperd√≠cio de ~1,325 samples (n√£o usa valida√ß√£o oficial no treino)

**Estrat√©gia**:

```
Original Training Set (5,216 samples)
    ‚Üì Split K=5 stratified
Fold 1: Train 4,173 | Val 1,043
Fold 2: Train 4,173 | Val 1,043
Fold 3: Train 4,173 | Val 1,043
Fold 4: Train 4,173 | Val 1,043
Fold 5: Train 4,173 | Val 1,043
    ‚Üì Aggregate
Final: Train em 100% | M√©tricas = m√©dia(5 folds) ¬± std
```

**Benef√≠cios**:

- ‚úÖ Valida√ß√£o em 1,043 samples (vs 16 atual) = 65√ó mais robusto
- ‚úÖ Early stopping confi√°vel
- ‚úÖ Intervalos de confian√ßa (95% CI) para todas as m√©tricas
- ‚úÖ Detec√ß√£o de overfitting mais precisa

**Implementa√ß√£o**:

```python
# src/cross_validation.py
from sklearn.model_selection import StratifiedKFold

def cross_validate_model(model_class, config, k=5):
    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)

    results = []
    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
        # Treinar modelo no fold
        model = train_fold(model_class, train_idx, val_idx, config)

        # Avaliar no test set (mesmo para todos os folds)
        metrics = evaluate(model, test_loader)
        results.append(metrics)

    # Agregar resultados
    mean_metrics = np.mean(results, axis=0)
    std_metrics = np.std(results, axis=0)

    return mean_metrics, std_metrics
```

**Tempo estimado**: 3-4 dias implementa√ß√£o + 40-50 horas treino (5 folds √ó 8-10h)
**Impacto esperado**: Accuracy +2-3%, m√©tricas mais confi√°veis
**Risco**: Baixo (metodologia padr√£o)

**Recomenda√ß√£o**: Executar em paralelo ou usar cloud computing (AWS, GCP) para acelerar.

---

#### TASK 4: Advanced Medical Augmentation ‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Simular varia√ß√µes realistas de raio-X para melhor generaliza√ß√£o

**Augmentations m√©dicos espec√≠ficos**:

1. **CLAHE (Contrast Limited Adaptive Histogram Equalization)**

   - Melhora contraste local sem amplificar ru√≠do
   - Simula varia√ß√µes de exposi√ß√£o de raio-X

   ```python
   transforms.Lambda(lambda img: clahe(img, clip_limit=2.0, tile_grid_size=(8,8)))
   ```

2. **Elastic Deformation**

   - Simula varia√ß√µes anat√¥micas (posicionamento do paciente)
   - Mant√©m estruturas anat√¥micas realistas

   ```python
   A.ElasticTransform(alpha=1, sigma=50, p=0.3)
   ```

3. **Grid Distortion**

   - Simula distor√ß√µes de lente/detector

   ```python
   A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3)
   ```

4. **Gaussian Noise**

   - Simula ru√≠do de detector

   ```python
   A.GaussNoise(var_limit=(10.0, 50.0), p=0.3)
   ```

5. **Brightness/Contrast**

   - Varia√ß√µes de exposi√ß√£o

   ```python
   A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5)
   ```

6. **Shift/Scale/Rotate**

   - Varia√ß√µes de posicionamento

   ```python
   A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5)
   ```

7. **Coarse Dropout**
   - Simula oclus√µes/artifacts
   ```python
   A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.3)
   ```

**Implementa√ß√£o completa**:

```python
# src/advanced_augmentation.py
import albumentations as A

def get_advanced_train_transform(image_size=224):
    return A.Compose([
        # Geometric
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5),
        A.HorizontalFlip(p=0.5),

        # Elastic & Grid
        A.ElasticTransform(alpha=1, sigma=50, p=0.3),
        A.GridDistortion(num_steps=5, distort_limit=0.3, p=0.3),

        # Intensity
        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=0.5),
        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
        A.RandomGamma(gamma_limit=(80, 120), p=0.3),

        # Noise & Quality
        A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),
        A.GaussianBlur(blur_limit=(3, 7), p=0.2),

        # Cutout/Dropout
        A.CoarseDropout(max_holes=8, max_height=32, max_width=32,
                        min_holes=1, min_height=8, min_width=8, p=0.3),

        # Final resize & normalize
        A.Resize(image_size, image_size),
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2(),
    ])
```

**Tempo estimado**: 2 dias implementa√ß√£o + 8-10 horas re-treinamento
**Impacto esperado**: Accuracy +3-5%, melhor generaliza√ß√£o
**Risco**: Baixo (augmentation √© padr√£o)

---

#### TASK 5: Test-Time Augmentation (TTA) ‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Reduzir vari√¢ncia de predi√ß√µes atrav√©s de m√∫ltiplas vers√µes da mesma imagem

**Conceito**:

```
Imagem original
    ‚Üì Gera N augmentations
Aug 1 ‚Üí Prediction p1
Aug 2 ‚Üí Prediction p2
...
Aug N ‚Üí Prediction pN
    ‚Üì Agregar
Final prediction = mean([p1, p2, ..., pN])
```

**Benef√≠cios**:

- ‚úÖ Predi√ß√µes mais est√°veis e confiantes
- ‚úÖ Reduz impacto de augmentations espec√≠ficos
- ‚úÖ Melhora AUC e calibra√ß√£o
- ‚úÖ N√£o requer re-treinamento!

**Implementa√ß√£o**:

```python
# src/tta.py
def predict_with_tta(model, image, n_augmentations=5):
    """
    Apply Test-Time Augmentation

    Args:
        model: Trained model
        image: Input image tensor [C, H, W]
        n_augmentations: Number of augmented versions (default 5)

    Returns:
        Average prediction across all augmentations
    """
    model.eval()
    predictions = []

    # Original image
    with torch.no_grad():
        output = model(image.unsqueeze(0))
        predictions.append(F.softmax(output, dim=1))

    # Augmented versions
    tta_transforms = [
        A.HorizontalFlip(p=1.0),
        A.Rotate(limit=5, p=1.0),
        A.Rotate(limit=-5, p=1.0),
        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0, rotate_limit=0, p=1.0),
        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),
    ]

    for transform in tta_transforms[:n_augmentations-1]:
        aug_image = transform(image=image.numpy())['image']
        aug_image = torch.from_numpy(aug_image)

        with torch.no_grad():
            output = model(aug_image.unsqueeze(0))
            predictions.append(F.softmax(output, dim=1))

    # Average predictions
    final_pred = torch.mean(torch.stack(predictions), dim=0)
    return final_pred
```

**Uso**:

```python
# Durante teste/infer√™ncia
for images, labels in test_loader:
    predictions = []
    for image in images:
        pred = predict_with_tta(model, image, n_augmentations=5)
        predictions.append(pred)
    # Avaliar
```

**Tempo estimado**: 1-2 dias implementa√ß√£o
**Impacto esperado**: AUC +0.01-0.02, Accuracy +1-2%
**Risco**: Muito baixo (apenas infer√™ncia)

---

### üü° PRIORIDADE M√âDIA (Melhorias Arquiteturais - 2-4 semanas)

#### TASK 6: Ensemble Inteligente - Stacking ‚≠ê‚≠ê‚≠ê‚≠ê

**Objetivo**: Superar melhor modelo individual atrav√©s de meta-learning

**Por que Simple/Weighted Voting falhou?**

- Modelos fracos (ResNet/DenseNet) "puxam para baixo"
- Pesos fixos n√£o se adaptam a caracter√≠sticas da imagem
- N√£o aprende quando confiar em cada modelo

**Solu√ß√£o: Stacked Generalization**

```
Level 0 (Base Models):
    EfficientNet-B0 ‚Üí Predictions P1
    ResNet-50       ‚Üí Predictions P2
    DenseNet-121    ‚Üí Predictions P3
            ‚Üì Concatenate
    Features: [P1, P2, P3]
            ‚Üì
Level 1 (Meta-Learner):
    Logistic Regression / XGBoost / LightGBM
            ‚Üì
    Final Prediction
```

**Meta-learner aprende**:

- Quando EfficientNet √© mais confi√°vel
- Quando ResNet detecta algo que outros perdem
- Padr√µes de concord√¢ncia/discord√¢ncia

**Implementa√ß√£o**:

```python
# src/stacking_ensemble.py
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
import lightgbm as lgb

class StackingEnsemble:
    def __init__(self, base_models, meta_model='logistic'):
        self.base_models = base_models

        if meta_model == 'logistic':
            self.meta_model = LogisticRegression(max_iter=1000)
        elif meta_model == 'xgboost':
            self.meta_model = GradientBoostingClassifier(n_estimators=100)
        elif meta_model == 'lightgbm':
            self.meta_model = lgb.LGBMClassifier(n_estimators=100)

    def fit(self, X_val, y_val):
        """
        Treinar meta-model nas predi√ß√µes dos base models
        """
        # Gerar predi√ß√µes de todos os base models
        base_predictions = []
        for model in self.base_models:
            model.eval()
            with torch.no_grad():
                preds = model(X_val)
                probs = F.softmax(preds, dim=1).cpu().numpy()
                base_predictions.append(probs)

        # Concatenar: shape (N_samples, N_models * N_classes)
        X_meta = np.hstack(base_predictions)

        # Treinar meta-model
        self.meta_model.fit(X_meta, y_val.cpu().numpy())

    def predict(self, X_test):
        """
        Predi√ß√£o com stacking
        """
        base_predictions = []
        for model in self.base_models:
            model.eval()
            with torch.no_grad():
                preds = model(X_test)
                probs = F.softmax(preds, dim=1).cpu().numpy()
                base_predictions.append(probs)

        X_meta = np.hstack(base_predictions)
        return self.meta_model.predict(X_meta)
```

**Variantes a testar**:

1. **Logistic Regression** (simples, interpret√°vel)
2. **XGBoost** (n√£o-linear, captura intera√ß√µes)
3. **LightGBM** (mais r√°pido, similar ao XGBoost)
4. **Neural Network** (1-2 camadas, m√°xima capacidade)

**Tempo estimado**: 3-5 dias
**Impacto esperado**: Accuracy +3-5% vs EfficientNet individual
**Risco**: M√©dio (pode overfittar se valida√ß√£o pequena - usar com cross-validation!)

---

#### TASK 7: Arquiteturas Modernas - Vision Transformer ‚≠ê‚≠ê‚≠ê

**Objetivo**: Capturar depend√™ncias long-range (contexto anat√¥mico global)

**Limita√ß√£o de CNNs**:

- Receptive field limitado
- Dificulta capturar rela√ß√µes entre regi√µes distantes (ex: cora√ß√£o + pulm√µes)

**Vantagem de Transformers**:

- Aten√ß√£o global desde a primeira camada
- Captura contexto completo da imagem

**Modelos recomendados**:

1. **ViT-Base** (86M params)
   - Vision Transformer original
   - Pr√©-treinado em ImageNet-21k
2. **Swin Transformer-Tiny** (28M params)
   - Shifted windows (efici√™ncia)
   - Hier√°rquico (multi-scale features)
3. **BEiT-Base** (86M params)

   - Self-supervised pr√©-training
   - Melhor para dom√≠nios espec√≠ficos

4. **ConvNeXt-Tiny** (28M params)
   - Hybrid CNN-Transformer
   - Efici√™ncia de CNN + capacidade de Transformer

**Implementa√ß√£o**:

```python
# src/models.py
import timm

def get_vision_transformer(model_name='vit_base_patch16_224', num_classes=2):
    """
    Load Vision Transformer
    """
    model = timm.create_model(
        model_name,
        pretrained=True,
        num_classes=num_classes
    )
    return model

# Op√ß√µes:
# - 'vit_base_patch16_224' (ViT-Base)
# - 'swin_tiny_patch4_window7_224' (Swin-Tiny)
# - 'beit_base_patch16_224' (BEiT-Base)
# - 'convnext_tiny' (ConvNeXt-Tiny)
```

**Estrat√©gia de treinamento**:

- Mesmo progressive unfreezing (5 √©pocas head-only + 20 full)
- Learning rate menor: 5e-5 (Transformers s√£o sens√≠veis)
- Gradient clipping: max_norm=1.0 (estabilidade)
- Warmup: 5 √©pocas com LR crescente (0 ‚Üí 5e-5)

**Tempo estimado**: 5-7 dias
**Impacto esperado**: Accuracy +2-5% (se dataset suficiente)
**Risco**: Alto (pode overfittar; requer mais dados ou regulariza√ß√£o forte)

---

#### TASK 8: Mixup / CutMix Augmentation ‚≠ê‚≠ê‚≠ê

**Objetivo**: Regulariza√ß√£o avan√ßada atrav√©s de interpola√ß√£o de exemplos

**Mixup**:

```python
# Interpola duas imagens
x_mixed = Œª * x_i + (1-Œª) * x_j
y_mixed = Œª * y_i + (1-Œª) * y_j

# Œª ~ Beta(Œ±, Œ±), Œ±=0.2
```

**CutMix**:

```python
# Recorta regi√£o de x_j e cola em x_i
x_cutmix = M ‚äô x_i + (1-M) ‚äô x_j
y_cutmix = Œª * y_i + (1-Œª) * y_j

# Œª = √°rea da regi√£o recortada / √°rea total
```

**Por que funciona?**

- For√ßa modelo a n√£o depender de regi√µes espec√≠ficas
- Reduz overfitting
- Melhora calibra√ß√£o (predi√ß√µes mais confiantes)

**Implementa√ß√£o**:

```python
# src/mixup_cutmix.py
def mixup_data(x, y, alpha=0.2):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]

    return mixed_x, y_a, y_b, lam

def cutmix_data(x, y, alpha=1.0):
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size(0)
    index = torch.randperm(batch_size).to(x.device)

    # Random box
    W, H = x.size(2), x.size(3)
    cut_w = int(W * np.sqrt(1 - lam))
    cut_h = int(H * np.sqrt(1 - lam))
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    x1 = np.clip(cx - cut_w // 2, 0, W)
    y1 = np.clip(cy - cut_h // 2, 0, H)
    x2 = np.clip(cx + cut_w // 2, 0, W)
    y2 = np.clip(cy + cut_h // 2, 0, H)

    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]

    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))
    y_a, y_b = y, y[index]

    return x, y_a, y_b, lam

# No training loop:
if use_mixup:
    inputs, targets_a, targets_b, lam = mixup_data(inputs, targets)
    outputs = model(inputs)
    loss = lam * criterion(outputs, targets_a) + (1-lam) * criterion(outputs, targets_b)
```

**Tempo estimado**: 2-3 dias
**Impacto esperado**: Accuracy +1-3%, melhor generaliza√ß√£o
**Risco**: Baixo

---

### üü¢ PRIORIDADE BAIXA (Otimiza√ß√µes Avan√ßadas - 1-2 meses)

#### TASK 9: Class-Balanced Loss ‚≠ê‚≠ê‚≠ê

**Objetivo**: Lidar com desbalanceamento atrav√©s de re-pondera√ß√£o baseada em frequ√™ncia efetiva

**Teoria**:
$$w_c = \frac{1 - \beta}{1 - \beta^{n_c}}$$

onde:

- $n_c$ = n√∫mero de samples da classe $c$
- $\beta \in [0, 1)$ (tipicamente 0.9999)

**Intui√ß√£o**: Classes raras t√™m peso maior, mas n√£o linear (evita overweight extremo)

**Implementa√ß√£o**:

```python
# src/losses.py
def get_class_balanced_weights(samples_per_class, beta=0.9999):
    effective_num = 1.0 - np.power(beta, samples_per_class)
    weights = (1.0 - beta) / effective_num
    weights = weights / weights.sum() * len(weights)
    return torch.tensor(weights, dtype=torch.float32)

# Uso:
samples_per_class = [1341, 3875]  # Normal, Pneumonia
cb_weights = get_class_balanced_weights(samples_per_class, beta=0.9999)
criterion = nn.CrossEntropyLoss(weight=cb_weights)
```

**Tempo estimado**: 1 dia + re-treinamento
**Impacto esperado**: Balanced Acc +2-4%
**Risco**: Baixo

---

#### TASK 10: Self-Supervised Pre-training ‚≠ê‚≠ê‚≠ê

**Objetivo**: Pr√©-treinar em dados de raio-X (sem labels) antes de fine-tuning

**Abordagens**:

1. **SimCLR**: Contrastive learning (imagens similares = embeddings pr√≥ximos)
2. **MoCo**: Momentum Contrast (queue de negative examples)
3. **BYOL**: Bootstrap Your Own Latent (sem negatives)
4. **MAE**: Masked Autoencoder (reconstruir patches mascarados)

**Por que funciona?**

- ImageNet tem fotos naturais; raio-X √© dom√≠nio diferente
- Self-supervised aprende features espec√≠ficas de raio-X
- Pode usar datasets grandes n√£o-anotados (ChestX-ray14, MIMIC-CXR)

**Pipeline**:

```
1. Coletar raio-X n√£o-anotados (100K-1M imagens)
2. Pr√©-treinar com SimCLR/MAE (1-2 semanas GPU)
3. Fine-tunar no nosso dataset (pneumonia)
4. Comparar com ImageNet pr√©-training
```

**Tempo estimado**: 3-4 semanas
**Impacto esperado**: Accuracy +3-7% (se dataset grande dispon√≠vel)
**Risco**: Alto (requer expertise, computa√ß√£o)

---

#### TASK 11: Ensemble de Ensembles ‚≠ê‚≠ê

**Objetivo**: Combinar m√∫ltiplos ensembles treinados com seeds diferentes

**Estrat√©gia**:

```
Seed 1: Train 3 models ‚Üí Ensemble 1
Seed 2: Train 3 models ‚Üí Ensemble 2
Seed 3: Train 3 models ‚Üí Ensemble 3
    ‚Üì Aggregate
Final: Average(Ensemble 1, Ensemble 2, Ensemble 3)
```

**Benef√≠cios**:

- Reduz vari√¢ncia de inicializa√ß√£o
- Mais robusto a outliers
- Melhora calibra√ß√£o

**Tempo estimado**: 1 semana (treino massivo)
**Impacto esperado**: Accuracy +1-2%
**Risco**: Alto (custo computacional)

---

## üéØ Roadmap Recomendado

### Fase 1: Quick Wins (1-2 semanas) - SEM RE-TREINAMENTO

**Objetivo**: Melhorias imediatas sem custo computacional

1. ‚úÖ **TASK 1: Threshold Optimization** (2-3 dias)

   - Implementar 4 m√©todos
   - Validar em test set
   - **Expectativa**: Spec 47% ‚Üí 62-65%, Sens 99% ‚Üí 95-97%

2. ‚úÖ **TASK 5: Test-Time Augmentation** (1-2 dias)
   - Implementar TTA com 5 augmentations
   - **Expectativa**: AUC +0.01-0.02, Acc +1-2%

**Resultado esperado Fase 1**:

- Accuracy: 80.29% ‚Üí **81-82%**
- Specificity: 47.86% ‚Üí **62-65%**
- Sensitivity: 99.74% ‚Üí **95-97%**
- Balanced Acc: 73.80% ‚Üí **78-81%**

---

### Fase 2: Re-training com Melhorias (2-3 semanas)

**Objetivo**: Re-treinar modelos com t√©cnicas avan√ßadas

3. ‚úÖ **TASK 2: Focal Loss** (1-2 dias + 10h treino)

   - Implementar Focal Loss (Œ≥=2.0)
   - Re-treinar EfficientNet-B0
   - **Expectativa**: Spec +8-12%

4. ‚úÖ **TASK 4: Advanced Augmentation** (2 dias + 10h treino)

   - Implementar 12+ augmentations
   - Re-treinar com novo pipeline
   - **Expectativa**: Acc +3-5%

5. ‚úÖ **TASK 3: Cross-Validation** (3-4 dias + 50h treino)
   - Implementar K=5 stratified CV
   - Treinar 5 modelos (paralelo se poss√≠vel)
   - **Expectativa**: M√©tricas mais confi√°veis, Acc +2-3%

**Resultado esperado Fase 2**:

- Accuracy: 82% ‚Üí **85-87%**
- Specificity: 65% ‚Üí **68-72%**
- Sensitivity: **95-97%** (mantido)
- Balanced Acc: 81% ‚Üí **83-86%**

---

### Fase 3: Ensemble Inteligente (1 semana)

**Objetivo**: Superar melhor modelo individual

6. ‚úÖ **TASK 6: Stacking Ensemble** (3-5 dias)
   - Treinar meta-learner (LightGBM)
   - Comparar com Simple/Weighted Voting
   - **Expectativa**: Acc +2-4% vs melhor individual

**Resultado esperado Fase 3**:

- Accuracy: 87% ‚Üí **88-90%**
- Specificity: **70-75%**
- Sensitivity: **95-97%**
- Balanced Acc: **86-88%**

---

### Fase 4: Arquiteturas Modernas (Opcional - 2-3 semanas)

**Objetivo**: Estado da arte absoluto

7. ‚úÖ **TASK 7: Vision Transformer** (5-7 dias)
   - Treinar Swin Transformer
   - Comparar com EfficientNet
8. ‚úÖ **TASK 8: Mixup/CutMix** (2-3 dias)
   - Adicionar ao pipeline de treino

**Resultado esperado Fase 4**:

- Accuracy: **90-92%**
- Specificity: **75-80%**
- Sensitivity: **95-97%**
- Balanced Acc: **88-90%**

---

## üìä Compara√ß√£o de Impacto vs Esfor√ßo

| Task                          | Impacto    | Esfor√ßo                     | Risco       | Prioridade |
| ----------------------------- | ---------- | --------------------------- | ----------- | ---------- |
| **1. Threshold Optimization** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Baixo (2-3 dias)            | Baixo       | üî¥ ALTA    |
| **2. Focal Loss**             | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√©dio (2 dias + 10h treino) | M√©dio       | üî¥ ALTA    |
| **3. Cross-Validation**       | ‚≠ê‚≠ê‚≠ê‚≠ê   | Alto (4 dias + 50h treino)  | Baixo       | üî¥ ALTA    |
| **4. Advanced Augmentation**  | ‚≠ê‚≠ê‚≠ê‚≠ê   | M√©dio (2 dias + 10h treino) | Baixo       | üî¥ ALTA    |
| **5. Test-Time Augmentation** | ‚≠ê‚≠ê‚≠ê‚≠ê   | Baixo (1-2 dias)            | Muito Baixo | üî¥ ALTA    |
| **6. Stacking Ensemble**      | ‚≠ê‚≠ê‚≠ê‚≠ê   | M√©dio (3-5 dias)            | M√©dio       | üü° M√âDIA   |
| **7. Vision Transformer**     | ‚≠ê‚≠ê‚≠ê     | Alto (5-7 dias)             | Alto        | üü° M√âDIA   |
| **8. Mixup/CutMix**           | ‚≠ê‚≠ê‚≠ê     | Baixo (2-3 dias)            | Baixo       | üü° M√âDIA   |
| **9. Class-Balanced Loss**    | ‚≠ê‚≠ê‚≠ê     | Baixo (1 dia + treino)      | Baixo       | üü¢ BAIXA   |
| **10. Self-Supervised**       | ‚≠ê‚≠ê‚≠ê     | Muito Alto (3-4 semanas)    | Alto        | üü¢ BAIXA   |
| **11. Ensemble de Ensembles** | ‚≠ê‚≠ê       | Muito Alto (1 semana)       | Alto        | üü¢ BAIXA   |

---

## ‚úÖ Checklist de Implementa√ß√£o

### Antes de come√ßar qualquer TASK:

- [ ] **Backup dos modelos atuais**

  ```bash
  cp -r models/ models_backup_$(date +%Y%m%d)/
  ```

- [ ] **Criar branch Git**

  ```bash
  git checkout -b improvements/task-name
  ```

- [ ] **Documentar baseline atual**

  - Salvar todas as m√©tricas atuais
  - Registrar hiperpar√¢metros
  - Anotar tempo de treinamento

- [ ] **Configurar logging detalhado**
  ```python
  import wandb  # ou TensorBoard
  wandb.init(project="pneumonia-improvements")
  ```

### Durante implementa√ß√£o:

- [ ] **Commits frequentes**

  ```bash
  git commit -m "feat: implement threshold optimization"
  ```

- [ ] **Valida√ß√£o incremental**

  - Testar cada componente isoladamente
  - Comparar com baseline ap√≥s cada mudan√ßa

- [ ] **Monitorar recursos**
  - GPU memory usage
  - Training time
  - Disk space

### Ap√≥s cada TASK:

- [ ] **An√°lise comparativa**

  - Gerar tabela comparativa (antes vs depois)
  - Calcular signific√¢ncia estat√≠stica
  - Visualizar m√©tricas

- [ ] **Documentar resultados**

  ```markdown
  ## TASK X: Nome

  - Implementa√ß√£o: [data]
  - Baseline: Acc=80.29%, Spec=47.86%
  - Resultado: Acc=85.12%, Spec=65.43%
  - Ganho: +4.83% Acc, +17.57% Spec
  - Tempo: 12h treino
  - Observa√ß√µes: ...
  ```

- [ ] **Merge se bem-sucedido**
  ```bash
  git checkout main
  git merge improvements/task-name
  git push
  ```

---

## üöÄ Pr√≥ximos Passos Imediatos

### Esta Semana (Come√ßar AGORA):

1. **Segunda-feira**: Implementar TASK 1 (Threshold Optimization)

   - C√≥digo: `src/threshold_optimization.py`
   - Testar 4 m√©todos
   - Validar em test set
   - **Meta**: Spec ‚â• 65%

2. **Ter√ßa-feira**: Implementar TASK 5 (TTA)

   - C√≥digo: `src/tta.py`
   - Testar com 5 augmentations
   - Medir impacto em AUC

3. **Quarta-Quinta**: Implementar TASK 2 (Focal Loss)

   - C√≥digo: `src/losses.py` (j√° existe!)
   - Modificar `train.py`
   - Iniciar re-treinamento

4. **Sexta**: An√°lise de resultados Fase 1
   - Comparar threshold optimization + TTA
   - Documentar ganhos
   - Decidir pr√≥ximos passos

### Pr√≥xima Semana:

5. **TASK 4**: Advanced Augmentation (2 dias)
6. **TASK 3**: Cross-Validation (iniciar - rodar em background/cloud)

---

## üìà Expectativas Finais

### Ap√≥s todas as melhorias (Fases 1-3):

| M√©trica              | Atual   | Meta         | Melhoria                    |
| -------------------- | ------- | ------------ | --------------------------- |
| **Accuracy**         | 80.29%  | **‚â• 88%**    | +7.71%                      |
| **AUC**              | 0.9761  | **‚â• 0.98**   | +0.004                      |
| **F1-Score**         | 0.8635  | **‚â• 0.90**   | +0.037                      |
| **Sensitivity**      | 99.74%  | **95-97%**   | -2-4% (trade-off aceit√°vel) |
| **Specificity**      | 47.86%  | **‚â• 70%**    | +22.14%                     |
| **Balanced Acc**     | 73.80%  | **‚â• 86%**    | +12.20%                     |
| **Falsos Positivos** | 122/234 | **‚â§ 70/234** | -52 casos                   |
| **Falsos Negativos** | 1/390   | **‚â§ 19/390** | +18 casos (aceit√°vel)       |

### Impacto cl√≠nico:

**Antes** (EfficientNet-B0 atual):

- ‚úÖ Excelente detec√ß√£o de pneumonia (99.74% sens)
- ‚ùå Muitos falsos alarmes (52% FPR)
- ‚ö†Ô∏è Sobrecarga de radiologistas

**Depois** (com todas as melhorias):

- ‚úÖ √ìtima detec√ß√£o de pneumonia (95-97% sens)
- ‚úÖ Falsos alarmes reduzidos (30% FPR)
- ‚úÖ Carga de trabalho vi√°vel
- ‚úÖ **Pronto para uso cl√≠nico com supervis√£o**

---

## üéì Refer√™ncias e Recursos

### Papers importantes:

1. **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017
2. **Mixup**: Zhang et al., "mixup: Beyond Empirical Risk Minimization", ICLR 2018
3. **CutMix**: Yun et al., "CutMix: Regularization Strategy to Train Strong Classifiers", ICCV 2019
4. **Class-Balanced Loss**: Cui et al., "Class-Balanced Loss Based on Effective Number of Samples", CVPR 2019
5. **Vision Transformer**: Dosovitskiy et al., "An Image is Worth 16x16 Words", ICLR 2021
6. **Swin Transformer**: Liu et al., "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows", ICCV 2021

### C√≥digo de refer√™ncia:

- **timm**: https://github.com/huggingface/pytorch-image-models
- **Albumentations**: https://albumentations.ai/
- **Focal Loss**: https://github.com/pytorch/vision/blob/main/torchvision/ops/focal_loss.py

---

## üí¨ Perguntas Frequentes

**Q: Devo implementar todas as tasks?**
A: N√£o! Siga o roadmap por fases. Fase 1-2 j√° deve alcan√ßar 85-87% accuracy.

**Q: E se Focal Loss n√£o funcionar?**
A: Tente Class-Balanced Loss (TASK 9) ou ajuste Œ≥ (testar 1.0, 1.5, 2.0, 2.5).

**Q: Cross-validation demora muito. Alternativas?**
A: Use apenas 3 folds (K=3) ou treine em cloud (AWS/GCP) com m√∫ltiplas GPUs.

**Q: Ensemble continua falhando. O que fazer?**
A: Implemente TASK 6 (Stacking). Se ainda falhar, foque em melhorar o EfficientNet individual.

**Q: Vale a pena usar Vision Transformer?**
A: Apenas se voc√™ tiver GPU potente (‚â• 16GB VRAM) ou cloud. EfficientNet + melhorias j√° alcan√ßa 85-88%.

**Q: Como sei se est√° funcionando?**
A: Monitore Balanced Accuracy e Specificity. Se ambos subirem, voc√™ est√° no caminho certo.

---

**Autor**: AI Assistant  
**Data**: 14 de Novembro de 2025  
**Vers√£o**: 1.0  
**Status**: Pronto para implementa√ß√£o
